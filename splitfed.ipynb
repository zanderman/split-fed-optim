{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SplitFed Model Optimization using Game Theoretic Approaches\n",
    "\n",
    "In this notebook we aim to optimize SplitFed ([arXiv:2004.12088](https://arxiv.org/abs/2004.12088)), a combination of Split Learning and Federated Learning ([arXiv:1810.06060](https://arxiv.org/abs/1810.06060), [arXiv:1812.00564](https://arxiv.org/abs/1812.00564)), using game theoretic approaches. Specifically, we look at balancing the number of model layers trained on each client device with computation overhead, communication overhead, and inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/a/64438413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import copy\n",
    "import glob\n",
    "import inspect\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tqdm\n",
    "from typing import Any, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set() # Use seaborn themes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "This section contains code that is modifies output path locations, random seed, and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds.\n",
    "SEED = 0\n",
    "tf.random.set_seed(SEED) # Only this works on ARC (since tensorflow==2.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging (useful for ARC systems).\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG) # Must be lowest of all handlers listed below.\n",
    "while logger.hasHandlers(): logger.removeHandler(logger.handlers[0]) # Clear all existing handlers.\n",
    "\n",
    "# Custom log formatting.\n",
    "formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "# Log to STDOUT (uses default formatting).\n",
    "sh = logging.StreamHandler(stream=sys.stdout)\n",
    "sh.setLevel(logging.INFO)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "# Set Tensorflow logging level.\n",
    "tf.get_logger().setLevel('ERROR') # 'INFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "# List all GPUs visible to TensorFlow.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "logger.info(f\"Num GPUs Available: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    logger.info(f\"Name: {gpu.name}, Type: {gpu.device_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Model Architecture\n",
    "\n",
    "To do Split Learning, a base model must be divided into client/server sub-models for training and evaluation. There are several configuration approaches to doing this as described in [arXiv:1812.00564](https://arxiv.org/abs/1812.00564). In this implementation, we focus on the simpler _vanilla_ configuration, which leverages a single forward/backward propagation pipeline. That is, the client model has a single input and the server holds the data labels. In the forward pass, data propagates through the client model, the outputs of which are then passed to the server where the loss is computed. In the backward pass, the gradients are computed at the server then backpropagated through its model, the final gradients are then sent to the client, where the backpropagation continues until the client input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 10)]              0         \n",
      "                                                                 \n",
      " layer1 (Dense)              (None, 2)                 22        \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 3)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31\n",
      "Trainable params: 31\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 3)]               0         \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 4)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16\n",
      "Trainable params: 16\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def split_model(\n",
    "    base_model: keras.models.Model,\n",
    "    cut_layer_key: int|str,\n",
    "    ) -> tuple[keras.models.Model, keras.models.Model]:\n",
    "\n",
    "    # Extract client-side input/output layers from base model.\n",
    "    inp_client = base_model.input\n",
    "    if isinstance(cut_layer_key, int):\n",
    "        out_client = base_model.get_layer(index=cut_layer_key).output\n",
    "    else:\n",
    "        out_client = base_model.get_layer(name=cut_layer_key).output\n",
    "\n",
    "    # Extract server-side output layer.\n",
    "    out_server = base_model.output\n",
    "\n",
    "    # Build client/server models.\n",
    "    model_client = keras.models.Model(inputs=inp_client, outputs=out_client)\n",
    "    model_server = keras.models.Model(inputs=out_client, outputs=out_server)\n",
    "    return model_server, model_client\n",
    "\n",
    "\n",
    "\n",
    "inp = keras.Input(shape=(10))\n",
    "x = keras.layers.Dense(2, activation=\"relu\", name=\"layer1\")(inp)\n",
    "x = keras.layers.Dense(3, activation=\"relu\", name=\"layer2\")(x)\n",
    "x = keras.layers.Dense(4, name=\"layer3\")(x)\n",
    "model = keras.Model(inputs=inp, outputs=x)\n",
    "s, c = split_model(model, 'layer2')\n",
    "c.summary()\n",
    "s.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Training Using Split Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def split_train_step(\n",
    "    model_server: keras.models.Model,\n",
    "    model_client: keras.models.Model,\n",
    "    x: tf.Tensor,\n",
    "    y: tf.Tensor,\n",
    "    ) -> dict[str, tf.Tensor]:\n",
    "    \"\"\"Compiled Split Learning training step.\n",
    "\n",
    "    Args:\n",
    "        model_server (keras.models.Model): Server model (compiled with optimizer and loss).\n",
    "        model_client (keras.models.Model): Client model (compiled with optimizer and loss).\n",
    "        x (tf.Tensor): Batched training input.\n",
    "        y (tf.Tensor): Batched training targets.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, tf.Tensor]: Dictionary of server model metrics after the current training step.\n",
    "    \"\"\"\n",
    "\n",
    "    # For this simulation we use a single GradientTape instance to make\n",
    "    # the codebase simpler. A true distributed environment would require\n",
    "    # a separate GradientTape instance for the server/client.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        ###### Client forward pass ######\n",
    "        out_client = model_client(x, training=True)\n",
    "\n",
    "        ###### Server forward pass ######\n",
    "        out_server = model_server(out_client, training=True)\n",
    "\n",
    "        ###### Server backward pass ######\n",
    "        loss = model_server.compiled_loss(\n",
    "            y_true=y,\n",
    "            y_pred=out_server,\n",
    "            regularization_losses=model_server.losses,\n",
    "        )\n",
    "        # Compute server gradients.\n",
    "        grad_server = tape.gradient(loss, model_server.trainable_variables)\n",
    "        # Update server weights.\n",
    "        model_server.optimizer.apply_gradients(zip(grad_server, model_server.trainable_variables))\n",
    "        # Update server metrics.\n",
    "        model_server.compiled_metrics.update_state(\n",
    "            y_true=y,\n",
    "            y_pred=out_server,\n",
    "        )\n",
    "\n",
    "        ###### Client backward pass ######\n",
    "        grad_client = tape.gradient(loss, model_client.trainable_variables)\n",
    "        # Update local client weights.\n",
    "        model_client.optimizer.apply_gradients(zip(grad_client, model_client.trainable_variables))\n",
    "        # No need to update client metrics since lables are on the server.\n",
    "\n",
    "    # Return dictionary of servermetrics (including loss).\n",
    "    return {m.name: m.result() for m in model_server.metrics}\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def split_test_step(\n",
    "    model_server: keras.models.Model,\n",
    "    model_client: keras.models.Model,\n",
    "    x: tf.Tensor,\n",
    "    y: tf.Tensor,\n",
    "    ) -> dict[str, tf.Tensor]:\n",
    "\n",
    "    ###### Client forward pass ######\n",
    "    out_client = model_client(x, training=False)\n",
    "\n",
    "    ###### Server forward pass ######\n",
    "    out_server = model_server(out_client, training=False)\n",
    "    # Update server metrics.\n",
    "    model_server.compiled_metrics.update_state(\n",
    "        y_true=y,\n",
    "        y_pred=out_server,\n",
    "    )\n",
    "\n",
    "    # Return dictionary of servermetrics (including loss).\n",
    "    return {m.name: m.result() for m in model_server.metrics}\n",
    "\n",
    "\n",
    "def fed_avg(\n",
    "    model_weights: dict[str, list[tf.Tensor]],\n",
    "    dist: dict[str, float],\n",
    "    ) -> list[tf.Tensor]:\n",
    "    \"\"\"Weighted average of model layer parameters.\n",
    "\n",
    "    Args:\n",
    "        model_weights (dict[str, list[tf.Tensor]]): Dictionary of model weight lists.\n",
    "        dist (dict[str, float]): Distribution for weighted averaging.\n",
    "\n",
    "    Returns:\n",
    "        list[tf.Tensor]: List of averaged weight tensors for each layer of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scale the weights using the given distribution.\n",
    "    model_weights_scaled = [\n",
    "        [dist[key] * layer for layer in weights] \n",
    "        for key, weights in model_weights.items()\n",
    "    ]\n",
    "\n",
    "    # Average the weights.\n",
    "    avg_weights = []\n",
    "    for weight_tup in zip(*model_weights_scaled):\n",
    "        avg_weights.append(\n",
    "            tf.math.reduce_sum(weight_tup, axis=0)\n",
    "        )\n",
    "    return avg_weights\n",
    "\n",
    "\n",
    "###\n",
    "# Vanilla SplitLearning configuration only.\n",
    "###\n",
    "def train_splitfed(\n",
    "    model_server: keras.models.Model,\n",
    "    model_client: keras.models.Model,\n",
    "    model_builder_server: Callable[[keras.models.Model], keras.models.Model],\n",
    "    model_builder_client: Callable[[keras.models.Model], keras.models.Model],\n",
    "    client_data: dict[int|str, tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]], # Dictionary of client data, where values are tuple(train, val, test) subsets (assumes already batched). The length of the dictionary determines the number of clients.\n",
    "    n_rounds: int, # Number of global communication rounds.\n",
    "    n_epochs: int, # Number of local client training epochs.\n",
    "    ):\n",
    "    # Determine number of clients.\n",
    "    n_clients: int = len(client_data)\n",
    "\n",
    "    ########## Main Server ###############\n",
    "    # Build initial server model.\n",
    "    model_server = model_builder_server(model_server)\n",
    "\n",
    "    # Copy of global server weight parameters.\n",
    "    global_weights_server = copy.deepcopy(model_server.get_weights())\n",
    "    ######################################\n",
    "\n",
    "    ########## Federated Server ##########\n",
    "    # Build initial client model.\n",
    "    model_client = model_builder_client(model_client)\n",
    "\n",
    "    # Copy of global client weight parameters.\n",
    "    global_weights_client = copy.deepcopy(model_client.get_weights())\n",
    "    #######################################\n",
    "\n",
    "    # Global training loop.\n",
    "    # Communication rounds between server <--> clients.\n",
    "    for round in range(n_rounds):\n",
    "        # Perserve server weights for each client update.\n",
    "        all_server_weights: dict[str, tf.Tensor] = {}\n",
    "\n",
    "        # Train each client model.\n",
    "        # This could be done in parallel, but here we do it \n",
    "        # synchronously for ease of development.\n",
    "        all_client_weights: dict[str, tf.Tensor] = {}\n",
    "        all_client_data_records_train: dict[str, int] = {}\n",
    "        for client, (train_dataset, val_dataset, test_dataset) in client_data.items():\n",
    "\n",
    "            # Reset server model so that weights are fresh during synchronous updates.\n",
    "            model_server.set_weights(global_weights_server)\n",
    "\n",
    "            # Synchronize global client model to local client.\n",
    "            model_client_local = model_builder_client(model_client)\n",
    "            model_client_local.set_weights(global_weights_client)\n",
    "\n",
    "            # Train the current model for the desired number of epochs.\n",
    "            all_client_data_records_train[client] = 0 # Initialize record count.\n",
    "            for epoch in range(n_epochs):\n",
    "\n",
    "                # Training loop.\n",
    "                with tqdm.tqdm(train_dataset, unit='batch') as pbar:\n",
    "                    for step, (x_train_batch, y_train_batch) in enumerate(pbar):\n",
    "                        pbar.set_description(f\"[round {round}/{n_rounds}, client {client}, epoch {epoch+1}/{n_epochs}] train:\")\n",
    "\n",
    "                        # Run a single training step.\n",
    "                        metrics_train = split_train_step(\n",
    "                            model_server=model_server,\n",
    "                            model_client=model_client_local,\n",
    "                            x=x_train_batch,\n",
    "                            y=y_train_batch,\n",
    "                        )\n",
    "\n",
    "                        # Add current number of batches to total number of records for the current client.\n",
    "                        all_client_data_records_train[client] += x_train_batch.shape[0]\n",
    "\n",
    "                        # Update progress bar with metrics.\n",
    "                        pbar.set_postfix(metrics_train)\n",
    "\n",
    "                # Validation loop.\n",
    "                with tqdm.tqdm(val_dataset, unit='batch') as pbar:\n",
    "                    for x_val_batch, y_val_batch in pbar:\n",
    "                        pbar.set_description(f\"[round {round}/{n_rounds}, client {client}, epoch {epoch+1}/{n_epochs}] val:\")\n",
    "\n",
    "                        # Run a single validation step.\n",
    "                        metrics_val = split_test_step(\n",
    "                            model_server=model_server,\n",
    "                            model_client=model_client_local,\n",
    "                            x=x_val_batch,\n",
    "                            y=y_val_batch,\n",
    "                        )\n",
    "\n",
    "                        # Update progress bar with metrics.\n",
    "                        pbar.set_postfix(metrics_val)\n",
    "\n",
    "                # Reset train/val metrics.\n",
    "                model_client.reset_metrics()\n",
    "                model_server.reset_metrics()\n",
    "\n",
    "            # Create a copy of this client's model weights and preserve for future aggregation.\n",
    "            all_client_weights[client] = copy.deepcopy(model_client_local.get_weights())\n",
    "\n",
    "            # Create a copy of the current server weights.\n",
    "            all_server_weights[client] = copy.deepcopy(model_server.get_weights())\n",
    "\n",
    "        # Count total number of data records across all clients.\n",
    "        total_data_records = float(sum(v for _, v in all_client_data_records_train.items()))\n",
    "\n",
    "        # Now perform federated averaging for weights of all clients.\n",
    "        # To do this, first cCompute distribution for weighted-average.\n",
    "        # Then perform federated averaging weight aggregation.\n",
    "        dist = {\n",
    "            client: float(count)/total_data_records\n",
    "            for client, count in all_client_data_records_train.items()\n",
    "        }\n",
    "        global_weights_client = fed_avg(model_weights=all_client_weights, dist=dist)\n",
    "\n",
    "        # Also average server weights for each client update.\n",
    "        global_weights_server = fed_avg(model_weights=all_server_weights, dist=dist)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e27c2da514be0f4555df3a4c15a4c6256ef40203ad64abea68e2343d203af1e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
