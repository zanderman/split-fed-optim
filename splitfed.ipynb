{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SplitFed Model Optimization using Game Theoretic Approaches\n",
    "\n",
    "In this notebook we aim to optimize SplitFed ([arXiv:2004.12088](https://arxiv.org/abs/2004.12088)), a combination of Split Learning and Federated Learning ([arXiv:1810.06060](https://arxiv.org/abs/1810.06060), [arXiv:1812.00564](https://arxiv.org/abs/1812.00564)), using game theoretic approaches. Specifically, we look at balancing the number of model layers trained on each client device with computation overhead, communication overhead, and inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/a/64438413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import copy\n",
    "import glob\n",
    "import inspect\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tqdm\n",
    "from typing import Any, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set() # Use seaborn themes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "This section contains code that is modifies output path locations, random seed, and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds.\n",
    "SEED = 0\n",
    "tf.random.set_seed(SEED) # Only this works on ARC (since tensorflow==2.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging (useful for ARC systems).\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG) # Must be lowest of all handlers listed below.\n",
    "while logger.hasHandlers(): logger.removeHandler(logger.handlers[0]) # Clear all existing handlers.\n",
    "\n",
    "# Custom log formatting.\n",
    "formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "# Log to STDOUT (uses default formatting).\n",
    "sh = logging.StreamHandler(stream=sys.stdout)\n",
    "sh.setLevel(logging.INFO)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "# Set Tensorflow logging level.\n",
    "tf.get_logger().setLevel('ERROR') # 'INFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "# List all GPUs visible to TensorFlow.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "logger.info(f\"Num GPUs Available: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    logger.info(f\"Name: {gpu.name}, Type: {gpu.device_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Model Architecture\n",
    "\n",
    "To do Split Learning, a base model must be divided into client/server sub-models for training and evaluation. There are several configuration approaches to doing this as described in [arXiv:1812.00564](https://arxiv.org/abs/1812.00564). In this implementation, we focus on the simpler _vanilla_ configuration, which leverages a single forward/backward propagation pipeline. That is, the client model has a single input and the server holds the data labels. In the forward pass, data propagates through the client model, the outputs of which are then passed to the server where the loss is computed. In the backward pass, the gradients are computed at the server then backpropagated through its model, the final gradients are then sent to the client, where the backpropagation continues until the client input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " layer1 (Dense)              (None, 2)                 22        \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 3)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31\n",
      "Trainable params: 31\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 3)]               0         \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 4)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16\n",
      "Trainable params: 16\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def split_model(\n",
    "    base_model: keras.models.Model,\n",
    "    cut_layer_key: int|str,\n",
    "    ) -> tuple[keras.models.Model, keras.models.Model]:\n",
    "\n",
    "    # Extract client-side input/output layers from base model.\n",
    "    inp_client = base_model.input\n",
    "    if isinstance(cut_layer_key, int):\n",
    "        out_client = base_model.get_layer(index=cut_layer_key).output\n",
    "    else:\n",
    "        out_client = base_model.get_layer(name=cut_layer_key).output\n",
    "\n",
    "    # Extract server-side output layer.\n",
    "    out_server = base_model.output\n",
    "\n",
    "    # Build client/server models.\n",
    "    model_client = keras.models.Model(inputs=inp_client, outputs=out_client)\n",
    "    model_server = keras.models.Model(inputs=out_client, outputs=out_server)\n",
    "    return model_server, model_client\n",
    "\n",
    "\n",
    "\n",
    "inp = keras.Input(shape=(10))\n",
    "x = keras.layers.Dense(2, activation=\"relu\", name=\"layer1\")(inp)\n",
    "x = keras.layers.Dense(3, activation=\"relu\", name=\"layer2\")(x)\n",
    "x = keras.layers.Dense(4, name=\"layer3\")(x)\n",
    "model = keras.Model(inputs=inp, outputs=x)\n",
    "s, c = split_model(model, 'layer2')\n",
    "c.summary()\n",
    "s.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Training Using Split Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_step(\n",
    "    model_server: keras.models.Model,\n",
    "    model_client: keras.models.Model,\n",
    "    x: tf.Tensor,\n",
    "    y: tf.Tensor,\n",
    "    ) -> dict[str, tf.Tensor]:\n",
    "    \"\"\"Split learning training step.\n",
    "\n",
    "    Runs a single training step for the given server and client models.\n",
    "\n",
    "    Note that the current implementation uses a single `tf.GradientTape` instance to\n",
    "    reduce code complexity. This means that the current implementation is for simulation\n",
    "    purposes only. True distributed learning would require a separate `tf.GradientTape`\n",
    "    instance for each model, where the backpropagation is done using a Jacobian matrix\n",
    "    across the separate tape gradients.\n",
    "\n",
    "    Args:\n",
    "        model_server (keras.models.Model): Server model (compiled with optimizer and loss).\n",
    "        model_client (keras.models.Model): Client model (compiled with optimizer and loss).\n",
    "        x (tf.Tensor): Batched training input.\n",
    "        y (tf.Tensor): Batched training targets.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, tf.Tensor]: Dictionary of server model metrics after the current training step.\n",
    "    \"\"\"\n",
    "\n",
    "    # For this simulation we use a single GradientTape instance to make\n",
    "    # the codebase simpler. A true distributed environment would require\n",
    "    # a separate GradientTape instance for the server/client.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        ###### Client forward pass ######\n",
    "        out_client = model_client(x, training=True)\n",
    "\n",
    "        ###### Server forward pass ######\n",
    "        out_server = model_server(out_client, training=True)\n",
    "\n",
    "        ###### Server backward pass ######\n",
    "        loss = model_server.compiled_loss(\n",
    "            y_true=y,\n",
    "            y_pred=out_server,\n",
    "            regularization_losses=model_server.losses,\n",
    "        )\n",
    "        # Compute server gradients.\n",
    "        grad_server = tape.gradient(loss, model_server.trainable_variables)\n",
    "        # Update server weights.\n",
    "        model_server.optimizer.apply_gradients(zip(grad_server, model_server.trainable_variables))\n",
    "        # Update server metrics.\n",
    "        model_server.compiled_metrics.update_state(\n",
    "            y_true=y,\n",
    "            y_pred=out_server,\n",
    "        )\n",
    "\n",
    "        ###### Client backward pass ######\n",
    "        grad_client = tape.gradient(loss, model_client.trainable_variables)\n",
    "        # Update local client weights.\n",
    "        model_client.optimizer.apply_gradients(zip(grad_client, model_client.trainable_variables))\n",
    "        # No need to update client metrics since lables are on the server.\n",
    "\n",
    "    # Return dictionary of servermetrics (including loss).\n",
    "    return {m.name: m.result() for m in model_server.metrics}\n",
    "\n",
    "\n",
    "def split_test_step(\n",
    "    model_server: keras.models.Model,\n",
    "    model_client: keras.models.Model,\n",
    "    x: tf.Tensor,\n",
    "    y: tf.Tensor,\n",
    "    ) -> dict[str, tf.Tensor]:\n",
    "    \"\"\"Split learning validation/test step.\n",
    "\n",
    "    Runs a single validation/test step for the given server and client models.\n",
    "\n",
    "    Args:\n",
    "        model_server (keras.models.Model): Server model (compiled with optimizer and loss).\n",
    "        model_client (keras.models.Model): Client model (compiled with optimizer and loss).\n",
    "        x (tf.Tensor): Batched validation/test input.\n",
    "        y (tf.Tensor): Batched validation/test targets.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, tf.Tensor]: Dictionary of server model metrics after the current validation/test step.\n",
    "    \"\"\"\n",
    "\n",
    "    ###### Client forward pass ######\n",
    "    out_client = model_client(x, training=False)\n",
    "\n",
    "    ###### Server forward pass ######\n",
    "    out_server = model_server(out_client, training=False)\n",
    "    # Update server metrics.\n",
    "    model_server.compiled_metrics.update_state(\n",
    "        y_true=y,\n",
    "        y_pred=out_server,\n",
    "    )\n",
    "\n",
    "    # Return dictionary of servermetrics (including loss).\n",
    "    return {f\"val_{m.name}\": m.result() for m in model_server.metrics}\n",
    "\n",
    "\n",
    "def fed_avg(\n",
    "    model_weights: dict[str, list[tf.Tensor]],\n",
    "    dist: dict[str, float],\n",
    "    ) -> list[tf.Tensor]:\n",
    "    \"\"\"Weighted average of model layer parameters.\n",
    "\n",
    "    Args:\n",
    "        model_weights (dict[str, list[tf.Tensor]]): Dictionary of model weight lists.\n",
    "        dist (dict[str, float]): Distribution for weighted averaging.\n",
    "\n",
    "    Returns:\n",
    "        list[tf.Tensor]: List of averaged weight tensors for each layer of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scale the weights using the given distribution.\n",
    "    model_weights_scaled = [\n",
    "        [dist[key] * layer for layer in weights] \n",
    "        for key, weights in model_weights.items()\n",
    "    ]\n",
    "\n",
    "    # Average the weights.\n",
    "    avg_weights = []\n",
    "    for weight_tup in zip(*model_weights_scaled):\n",
    "        avg_weights.append(\n",
    "            tf.math.reduce_sum(weight_tup, axis=0)\n",
    "        )\n",
    "    return avg_weights\n",
    "\n",
    "\n",
    "###\n",
    "# Vanilla SplitLearning configuration only.\n",
    "###\n",
    "def train_splitfed(\n",
    "    model_server: keras.models.Model,\n",
    "    model_client: keras.models.Model,\n",
    "    model_builder_server: Callable[[keras.models.Model], keras.models.Model],\n",
    "    model_builder_client: Callable[[keras.models.Model], keras.models.Model],\n",
    "    client_data: dict[int|str, tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]], # Dictionary of client data, where values are tuple(train, val, test) subsets (assumes already batched). The length of the dictionary determines the number of clients.\n",
    "    n_rounds: int, # Number of global communication rounds.\n",
    "    n_epochs: int, # Number of local client training epochs.\n",
    "    ) -> tuple[keras.models.Model, keras.models.Model]:\n",
    "    # Determine number of clients.\n",
    "    n_clients: int = len(client_data)\n",
    "\n",
    "    ########## Main Server ###############\n",
    "    # Build initial server model.\n",
    "    model_server = model_builder_server(model_server)\n",
    "\n",
    "    # Copy of global server weight parameters.\n",
    "    global_weights_server = copy.deepcopy(model_server.get_weights())\n",
    "    ######################################\n",
    "\n",
    "    ########## Federated Server ##########\n",
    "    # Build initial client model.\n",
    "    model_client = model_builder_client(model_client)\n",
    "\n",
    "    # Copy of global client weight parameters.\n",
    "    global_weights_client = copy.deepcopy(model_client.get_weights())\n",
    "    #######################################\n",
    "\n",
    "    # Global training loop.\n",
    "    # Communication rounds between server <--> clients.\n",
    "    for round in range(n_rounds):\n",
    "        # Perserve server weights for each client update.\n",
    "        all_server_weights: dict[str, tf.Tensor] = {}\n",
    "\n",
    "        # Train each client model.\n",
    "        # This could be done in parallel, but here we do it \n",
    "        # synchronously for ease of development.\n",
    "        all_client_weights: dict[str, tf.Tensor] = {}\n",
    "        all_client_data_records_train: dict[str, int] = {}\n",
    "        for client, (train_dataset, val_dataset, test_dataset) in client_data.items():\n",
    "\n",
    "            # Reset server model so that weights are fresh during synchronous updates.\n",
    "            model_server.set_weights(global_weights_server)\n",
    "\n",
    "            # Synchronize global client model to local client.\n",
    "            model_client_local = model_builder_client(model_client)\n",
    "            model_client_local.set_weights(global_weights_client)\n",
    "\n",
    "            # Train the current model for the desired number of epochs.\n",
    "            all_client_data_records_train[client] = 0 # Initialize record count.\n",
    "            for epoch in range(n_epochs):\n",
    "\n",
    "                # Training loop.\n",
    "                with tqdm.tqdm(train_dataset, unit='batch') as pbar:\n",
    "                    for step, (x_train_batch, y_train_batch) in enumerate(pbar):\n",
    "                        pbar.set_description(f\"[round {round+1}/{n_rounds}, client {client}, epoch {epoch+1}/{n_epochs}] train\")\n",
    "\n",
    "                        # Run a single training step.\n",
    "                        metrics_train = split_train_step(\n",
    "                            model_server=model_server,\n",
    "                            model_client=model_client_local,\n",
    "                            x=x_train_batch,\n",
    "                            y=y_train_batch,\n",
    "                        )\n",
    "\n",
    "                        # Add current number of batches to total number of records for the current client.\n",
    "                        all_client_data_records_train[client] += x_train_batch.shape[0]\n",
    "\n",
    "                        # Update progress bar with metrics.\n",
    "                        pbar.set_postfix({k:v.numpy() for k,v in metrics_train.items()})\n",
    "\n",
    "                # Validation loop.\n",
    "                with tqdm.tqdm(val_dataset, unit='batch') as pbar:\n",
    "                    for x_val_batch, y_val_batch in pbar:\n",
    "                        pbar.set_description(f\"[round {round+1}/{n_rounds}, client {client}, epoch {epoch+1}/{n_epochs}] val\")\n",
    "\n",
    "                        # Run a single validation step.\n",
    "                        metrics_val = split_test_step(\n",
    "                            model_server=model_server,\n",
    "                            model_client=model_client_local,\n",
    "                            x=x_val_batch,\n",
    "                            y=y_val_batch,\n",
    "                        )\n",
    "\n",
    "                        # Update progress bar with metrics.\n",
    "                        pbar.set_postfix({k:v.numpy() for k,v in metrics_val.items()})\n",
    "\n",
    "                # Reset train/val metrics.\n",
    "                model_client.reset_metrics()\n",
    "                model_server.reset_metrics()\n",
    "\n",
    "            # Create a copy of this client's model weights and preserve for future aggregation.\n",
    "            all_client_weights[client] = copy.deepcopy(model_client_local.get_weights())\n",
    "\n",
    "            # Create a copy of the current server weights.\n",
    "            all_server_weights[client] = copy.deepcopy(model_server.get_weights())\n",
    "\n",
    "        # Count total number of data records across all clients.\n",
    "        total_data_records = float(sum(v for _, v in all_client_data_records_train.items()))\n",
    "\n",
    "        # Now perform federated averaging for weights of all clients.\n",
    "        # To do this, first cCompute distribution for weighted-average.\n",
    "        # Then perform federated averaging weight aggregation.\n",
    "        dist = {\n",
    "            client: float(count)/total_data_records\n",
    "            for client, count in all_client_data_records_train.items()\n",
    "        }\n",
    "        global_weights_client = fed_avg(model_weights=all_client_weights, dist=dist)\n",
    "\n",
    "        # Also average server weights for each client update.\n",
    "        global_weights_server = fed_avg(model_weights=all_server_weights, dist=dist)\n",
    "\n",
    "    # Load the final global weights for the server and client.\n",
    "    model_server.set_weights(global_weights_server)\n",
    "    model_client.set_weights(global_weights_client)\n",
    "\n",
    "    # Return server and client models.\n",
    "    return model_server, model_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.data.experimental.cardinality(train_dataset).numpy()=782\n",
      "tf.data.experimental.cardinality(val_dataset).numpy()=79\n",
      "tf.data.experimental.cardinality(test_dataset).numpy()=79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[round 1/1, client 0, epoch 1/1] train: 100%|██████████| 782/782 [00:11<00:00, 67.03batch/s, loss=2.27, acc=0.823]\n",
      "[round 1/1, client 0, epoch 1/1] val: 100%|██████████| 79/79 [00:00<00:00, 204.58batch/s, val_loss=2.27, val_acc=0.827]\n",
      "[round 1/1, client 1, epoch 1/1] train: 100%|██████████| 782/782 [00:11<00:00, 67.87batch/s, loss=2, acc=0.822]   \n",
      "[round 1/1, client 1, epoch 1/1] val: 100%|██████████| 79/79 [00:00<00:00, 196.56batch/s, val_loss=2, val_acc=0.827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " digits (InputLayer)         [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense0 (Dense)              (None, 64)                50240     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,240\n",
      "Trainable params: 50,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 64)]              0         \n",
      "                                                                 \n",
      " dense1 (Dense)              (None, 64)                4160      \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,810\n",
      "Trainable params: 4,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))\n",
    "\n",
    "# Reserve 5,000 samples for validation, and 5,000 for testing.\n",
    "x_test = x_train[-5000:]\n",
    "y_test = y_train[-5000:]\n",
    "x_val = x_train[-10000:-5000]\n",
    "y_val = y_train[-10000:-5000]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Prepare the testing dataset.\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "def compile_model(model: keras.models.Model):\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['acc'],\n",
    "        )\n",
    "    return model\n",
    "\n",
    "def build_model():\n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    x1 = keras.layers.Dense(64, activation=\"relu\", name='dense0')(inputs)\n",
    "    x2 = keras.layers.Dense(64, activation=\"relu\", name='dense1')(x1)\n",
    "    outputs = keras.layers.Dense(10, name=\"predictions\")(x2)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    server, client = split_model(model, 'dense0')\n",
    "    return server, client\n",
    "\n",
    "print(f\"{tf.data.experimental.cardinality(train_dataset).numpy()=}\")\n",
    "print(f\"{tf.data.experimental.cardinality(val_dataset).numpy()=}\")\n",
    "print(f\"{tf.data.experimental.cardinality(test_dataset).numpy()=}\")\n",
    "\n",
    "\n",
    "# THIS IS NAIVE, NEEDS TO BE CORRECTED!\n",
    "# Build client datasets.\n",
    "n_clients = 2\n",
    "client_data = {\n",
    "    c: (train_dataset, val_dataset, test_dataset)\n",
    "    for c in range(n_clients)\n",
    "}\n",
    "\n",
    "\n",
    "server, client = build_model()\n",
    "\n",
    "server_trained, client_trained = train_splitfed(\n",
    "    model_server=server,\n",
    "    model_client=client,\n",
    "    model_builder_server=compile_model,\n",
    "    model_builder_client=compile_model,\n",
    "    client_data=client_data, # Dictionary of client data, where values are tuple(train, val, test) subsets (assumes already batched). The length of the dictionary determines the number of clients.\n",
    "    n_rounds=1, # Number of global communication rounds.\n",
    "    n_epochs=1, # Number of local client training epochs.\n",
    ")\n",
    "\n",
    "client.summary()\n",
    "server.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Theory Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_forward_prop(\n",
    "    d: float, # Portion of network that client device `k` will train.\n",
    "    W: float, # Total size of neural network to be trained.\n",
    "    D: float, # Data size for client `k`.\n",
    "    beta: float, # Amount of computational complexity of forward propagation.\n",
    "    F_dk: float, # The computing resource of device `k`.\n",
    "    ) -> float:\n",
    "    \"\"\"Computes `T_{d,k}^{F}`, the computing time for forward propagation of device `k`.\n",
    "\n",
    "    Mathematically, this is:\n",
    "        `T_{d,k}^{F} = (d * beta * W * D) / F_{d,k}`\n",
    "\n",
    "    Args:\n",
    "        d (float): Portion of network that client device `k` will train.\n",
    "        W (float): Total size of neural network to be trained.\n",
    "        D (float): Data size for client `k`.\n",
    "        beta (float): Amount of computational complexity forward propagation.\n",
    "        F_dk (float): The computing resource of device `k`, called `F_{d,k}`.\n",
    "\n",
    "    Returns:\n",
    "        float: `T_{d,k}^{F}`\n",
    "    \"\"\"\n",
    "    return (d*beta*W*D)/F_dk\n",
    "\n",
    "def compute_time_backward_prop(\n",
    "    d: float, # Portion of network that client device `k` will train.\n",
    "    W: float, # Total size of neural network to be trained.\n",
    "    D: float, # Data size for client `k`.\n",
    "    beta: float, # Amount of computational complexity of forward propagation.\n",
    "    F_dk: float, # The computing resource of device `k`.\n",
    "    ) -> float:\n",
    "    \"\"\"Computes `T_{d,k}^{B}`, the computing time for backward propagation of device `k`.\n",
    "\n",
    "    Mathematically, this is:\n",
    "        `T_{d,k}^{B} = (d * (1-beta) * W * D) / F_{d,k}`\n",
    "\n",
    "    Args:\n",
    "        d (float): Portion of network that client device `k` will train.\n",
    "        W (float): Total size of neural network to be trained.\n",
    "        D (float): Data size for client `k`.\n",
    "        beta (float): Amount of computational complexity forward propagation.\n",
    "        F_dk (float): The computing resource of device `k`, called `F_{d,k}`.\n",
    "\n",
    "    Returns:\n",
    "        float: `T_{d,k}^{B}`\n",
    "    \"\"\"\n",
    "    return (d*(1-beta)*W*D)/F_dk\n",
    "\n",
    "def compute_time_client(\n",
    "    d: float, # Portion of network that client device `k` will train.\n",
    "    W: float, # Total size of neural network to be trained.\n",
    "    D: float, # Data size for client `k`.\n",
    "    beta: float, # Amount of computational complexity of forward propagation.\n",
    "    F_dk: float, # The computing resource of device `k`.\n",
    "    ) -> float:\n",
    "    \"\"\"Computes `T_{d,k}`, the total computing time of device `k`.\n",
    "\n",
    "    Mathematically, this is:\n",
    "        `T_{d,k} = T_{d,k}^{F} + T_{d,k}^{B}`\n",
    "\n",
    "    Args:\n",
    "        d (float): Portion of network that client device `k` will train.\n",
    "        W (float): Total size of neural network to be trained.\n",
    "        D (float): Data size for client `k`.\n",
    "        beta (float): Amount of computational complexity forward propagation.\n",
    "        F_dk (float): The computing resource of device `k`, called `F_{d,k}`.\n",
    "\n",
    "    Returns:\n",
    "        float: `T_{d,k}`\n",
    "    \"\"\"\n",
    "    T_F = compute_time_forward_prop(\n",
    "        d=d,\n",
    "        W=W,\n",
    "        D=D,\n",
    "        beta=beta,\n",
    "        F_dk=F_dk,\n",
    "    )\n",
    "    T_B = compute_time_backward_prop(\n",
    "        d=d,\n",
    "        W=W,\n",
    "        D=D,\n",
    "        beta=beta,\n",
    "        F_dk=F_dk,\n",
    "    )\n",
    "    return T_F + T_B\n",
    "\n",
    "def compute_time_server(\n",
    "    d: float, # Portion of network that client device `k` will train.\n",
    "    W: float, # Total size of neural network to be trained.\n",
    "    D: float, # Data size for client `k`.\n",
    "    F_s: float, # The computing resource of the server.\n",
    "    ) -> float:\n",
    "    \"\"\"Computes `T_{s}`, the computing time of the server `s` for a single client.\n",
    "\n",
    "    Mathematically, this is:\n",
    "        `T_{s} = ((1 - d) * W * D) / F_{s}`\n",
    "\n",
    "    Args:\n",
    "        d (float): Portion of network that the client device will train.\n",
    "        W (float): Total size of neural network to be trained.\n",
    "        D (float): Data size for client `k`.\n",
    "        F_s (float): The computing resource of server `s`.\n",
    "\n",
    "    Returns:\n",
    "        float: `T_{s}`\n",
    "    \"\"\"\n",
    "    return ((1-d)*W*D)/F_s\n",
    "\n",
    "def compute_time_global_epoch(\n",
    "    d: list[float], # Portion of network that client device `k` will train.\n",
    "    W: float, # Total size of neural network to be trained.\n",
    "    D: list[float], # Data size for each client `k`.\n",
    "    beta: float, # Amount of computational complexity of forward propagation.\n",
    "    F_dk: list[float], # The computing resource of each client `k`.\n",
    "    F_s: float, # The computing resource of the server.\n",
    "    ) -> float:\n",
    "    \"\"\"Computes `T_{g}`, the total computing time of one global epoch with 1 server and a subset of `k` clients.\n",
    "\n",
    "    Mathematically, this is:\n",
    "        `T_{g} = \\max_{k}{T_{d,k}^{F}} + \\max_{k}{T_{d,k}^{B}} + \\sum_{i=1}^{k}{T_{s}}`\n",
    "\n",
    "    Args:\n",
    "        d (list[float]): Portions of network that each client device `k` will train.\n",
    "        W (float): Total size of neural network to be trained.\n",
    "        D (list[float]): Data size.\n",
    "        beta (float): Amount of computational complexity forward propagation.\n",
    "        F_dk (list[float]): The computing resource of device `k`, called `F_{d,k}`.\n",
    "        F_s (float): The computing resource of server `s`.\n",
    "\n",
    "    Returns:\n",
    "        float: `T_{g}`\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays for vectorization.\n",
    "    d = np.array(d)\n",
    "    D = np.array(D)\n",
    "    F_dk = np.array(F_dk)\n",
    "\n",
    "    # Ensure all are same length.\n",
    "    assert len(d) == len(D) == len(F_dk)\n",
    "\n",
    "    # Compute maximum client compute time.\n",
    "    max_T_dk = np.max(compute_time_client(\n",
    "        d=d,\n",
    "        W=W,\n",
    "        D=D,\n",
    "        beta=beta,\n",
    "        F_dk=F_dk,\n",
    "    ))\n",
    "\n",
    "    # Compute total server compute time.\n",
    "    sum_T_s = np.sum(compute_time_server(\n",
    "        d=d,\n",
    "        W=W,\n",
    "        D=D,\n",
    "        F_s=F_s,\n",
    "    ))\n",
    "\n",
    "    # Compute total time for one global epoch.\n",
    "    return max_T_dk + sum_T_s\n",
    "\n",
    "def utility_client(\n",
    "    d: float, # Portion of network that client device `k` will train.\n",
    "    W: float, # Total size of neural network to be trained.\n",
    "    D: float, # Data size for client `k`.\n",
    "    F_dk: float, # The computing resource of device `k`.\n",
    "    K: float, # ?\n",
    "    C_k: float, # ?\n",
    "    lam: float, # Discount factor.\n",
    "    ) -> float:\n",
    "    \"\"\"Computes `U_{d,k}`, the utility of client `k`.\n",
    "\n",
    "    Mathematically, this is:\n",
    "        `U_{d,k} = C_{k} * F_{d,k} - d * W * D * k * (F_{d,k}^2) + lam * \\log_{2}{1 + d}`\n",
    "\n",
    "    Args:\n",
    "        d (float): Portion of network that client device `k` will train.\n",
    "        W (float): Total size of neural network to be trained.\n",
    "        D (float): Data size for client `k`.\n",
    "        beta (float): Amount of computational complexity forward propagation.\n",
    "        F_dk (float): The computing resource of device `k`, called `F_{d,k}`.\n",
    "        K (float): ?\n",
    "        C_k (float): ?\n",
    "        lam (float): Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        float: `U_{d,k}`\n",
    "    \"\"\"\n",
    "    reward_server = C_k * F_dk\n",
    "    energy_consume_train = d * W * D * K * (F_dk**2.0)\n",
    "    reward_privacy = lam * np.log2(1 + d)\n",
    "    return reward_server - energy_consume_train + reward_privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-323301.0\n"
     ]
    }
   ],
   "source": [
    "x = compute_time_global_epoch(\n",
    "    d=np.arange(1,100), # Portion of network that client device `k` will train.\n",
    "    W=1.0, # Total size of neural network to be trained.\n",
    "    D=np.arange(1,100), # Data size.\n",
    "    beta=1.0, # Amount of computational complexity of forward propagation.\n",
    "    F_dk=np.arange(1,100), # The computing resource of device `k`.\n",
    "    F_s=1.0, # The computing resource of the server.\n",
    ")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e27c2da514be0f4555df3a4c15a4c6256ef40203ad64abea68e2343d203af1e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
