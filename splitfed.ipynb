{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SplitFed Model Optimization using Game Theoretic Approaches\n",
    "\n",
    "In this notebook we aim to optimize SplitFed ([arXiv:2004.12088](https://arxiv.org/abs/2004.12088)), a combination of Split Learning and Federated Learning ([arXiv:1810.06060](https://arxiv.org/abs/1810.06060), [arXiv:1812.00564](https://arxiv.org/abs/1812.00564)), using game theoretic approaches. Specifically, we look at balancing the number of model layers trained on each client device with computation overhead, communication overhead, and inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/a/64438413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import glob\n",
    "import inspect\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set() # Use seaborn themes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "This section contains code that is modifies output path locations, random seed, and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds.\n",
    "SEED = 0\n",
    "tf.random.set_seed(SEED) # Only this works on ARC (since tensorflow==2.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging (useful for ARC systems).\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG) # Must be lowest of all handlers listed below.\n",
    "while logger.hasHandlers(): logger.removeHandler(logger.handlers[0]) # Clear all existing handlers.\n",
    "\n",
    "# Custom log formatting.\n",
    "formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "# Log to STDOUT (uses default formatting).\n",
    "sh = logging.StreamHandler(stream=sys.stdout)\n",
    "sh.setLevel(logging.INFO)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "# Set Tensorflow logging level.\n",
    "tf.get_logger().setLevel('ERROR') # 'INFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "# List all GPUs visible to TensorFlow.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "logger.info(f\"Num GPUs Available: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    logger.info(f\"Name: {gpu.name}, Type: {gpu.device_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Model Architecture\n",
    "\n",
    "To do Split Learning, a base model must be divided into client/server sub-models for training and evaluation. There are several configuration approaches to doing this as described in [arXiv:1812.00564](https://arxiv.org/abs/1812.00564). In this implementation, we focus on the simpler _vanilla_ configuration, which leverages a single forward/backward propagation pipeline. That is, the client model has a single input and the server holds the data labels. In the forward pass, data propagates through the client model, the outputs of which are then passed to the server where the loss is computed. In the backward pass, the gradients are computed at the server then backpropagated through its model, the final gradients are then sent to the client, where the backpropagation continues until the client input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 10)]              0         \n",
      "                                                                 \n",
      " layer1 (Dense)              (None, 2)                 22        \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 3)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31\n",
      "Trainable params: 31\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 3)]               0         \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 4)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16\n",
      "Trainable params: 16\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def split_model(\n",
    "    base_model: keras.models.Model,\n",
    "    cut_layer_key: int|str,\n",
    "    ) -> tuple[keras.models.Model, keras.models.Model]:\n",
    "\n",
    "    # Extract client-side input/output layers from base model.\n",
    "    inp_client = base_model.input\n",
    "    if isinstance(cut_layer_key, int):\n",
    "        out_client = base_model.get_layer(index=cut_layer_key).output\n",
    "    else:\n",
    "        out_client = base_model.get_layer(name=cut_layer_key).output\n",
    "\n",
    "    # Extract server-side output layer.\n",
    "    out_server = base_model.output\n",
    "\n",
    "    # Build client/server models.\n",
    "    model_client = keras.models.Model(inputs=inp_client, outputs=out_client)\n",
    "    model_server = keras.models.Model(inputs=out_client, outputs=out_server)\n",
    "    return model_server, model_client\n",
    "\n",
    "\n",
    "\n",
    "inp = keras.Input(shape=(10))\n",
    "x = keras.layers.Dense(2, activation=\"relu\", name=\"layer1\")(inp)\n",
    "x = keras.layers.Dense(3, activation=\"relu\", name=\"layer2\")(x)\n",
    "x = keras.layers.Dense(4, name=\"layer3\")(x)\n",
    "model = keras.Model(inputs=inp, outputs=x)\n",
    "s, c = split_model(model, 'layer2')\n",
    "c.summary()\n",
    "s.summary()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e27c2da514be0f4555df3a4c15a4c6256ef40203ad64abea68e2343d203af1e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
